{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "933f8bb0",
   "metadata": {
    "id": "933f8bb0"
   },
   "source": [
    "# Machine Learning Final Project\n",
    "\n",
    "## Submitted by: Pratik Shukla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e2975",
   "metadata": {
    "id": "696e2975"
   },
   "source": [
    "# Questions\n",
    "\n",
    "## 1. Your views about the problem statement?\n",
    "The problem statement is about companie named TechWorks Consulting wants to create a machine learning model to determine the salary of newly hired employees. It is important to companies to set competitive and fair salary to employees. Machine Learning model can provide objective on this problem based on many variables like college, Experience, Role, previous CTC and academic record. As a data scientist my role is to make a effective machine learning model or predictive model and evaluate its performance.\n",
    "\n",
    "\n",
    "## 2. What will be your approach to solving this task?\n",
    "\n",
    "My approach to solving this task is as follows:\n",
    "\n",
    "**(A) Data Preprocessing:**\n",
    "- Convert \"College\" into a numerical data type based on the tier of the college.\n",
    "- Convert the \"City\" field into numerical data (0 for non-metro, 1 for metro cities).\n",
    "- Create dummy variables for the \"Tier\" and \"Role\" fields.\n",
    "- Perform Exploratory Data Analysis (EDA) and check for null values.\n",
    "- Visualize various graphs to find outliers.\n",
    "- Identify outliers using percentiles (e.g., 99th and 1st percentiles) and consider them as potential outliers.\n",
    "- Treat outliers by replacing them with the mean if outliers are present.\n",
    "\n",
    "**(B) Model Selection:**\n",
    "- Choose an appropriate regression model for predicting salary. Consider regression models like Linear Regression, Ridge Regression, Lasso Regression, Decision Tree Regression, Random Forest Regressor, XG Boost Regressor, Bagging Regressor.\n",
    "- I also do Standardisation of data for models\n",
    "- Try multiple models to assess their performance.\n",
    "\n",
    "**(C) Model Training and Evaluation:**\n",
    "- Split the data into test and train datasets.\n",
    "- Train the particular model on the training dataset.\n",
    "- Evaluate the model's performance using metrics like R-Squared and Mean Squared Error.\n",
    "- Select the model with the best performance.\n",
    "\n",
    "**(D) Model Optimization:**\n",
    "- Optimize the model with cross-validation and hyperparameters.\n",
    "\n",
    "   - For Ridge and Lasso Regression, use the validation curve to tune the hyperparameters.\n",
    "   - For Decision Tree Regression, Random Forest, XGB Regressor, Bagging Regressor use GridSearchCV with parameter grids for max depth, min sample split, and min sample leaf.\n",
    "   \n",
    "**(E) Conclusion or Result:**\n",
    "- After performing the all model and get the result I will choose the best model according to R2 score and Mean Squared Error (MSE). Model which have High R2 score and Less MSE is the best one.  \n",
    "\n",
    "## **3. What were the available ML model options you had to perform this task?**\n",
    "\n",
    "For predicting employee salaries, we considered various regression models:\n",
    "\n",
    "For predicting employee salaries, we considered a range of regression models. Each model offers unique characteristics and advantages for different scenarios:\n",
    "\n",
    "1. **Simple Linear Regression:**\n",
    "   - Suitable when there's a straightforward, linear relationship between one feature and salary.\n",
    "\n",
    "2. **Multiple Linear Regression:**\n",
    "   - Useful when multiple features influence salary, allowing for more complex relationships to be considered.\n",
    "\n",
    "3. **Ridge Regression:**\n",
    "   - Prevents overfitting by adding a penalty term, making it ideal for handling multicollinearity among features.\n",
    "\n",
    "4. **Lasso Regression:**\n",
    "   - Combats overfitting and aids feature selection by encouraging some features to have zero influence on salary prediction.\n",
    "\n",
    "5. **Decision Tree Regression:**\n",
    "   - A non-linear model that captures complex relationships when the salary prediction is not linear.\n",
    "\n",
    "6. **Random Forest Regressor:**\n",
    "   - An ensemble learning model that combines multiple decision trees to improve predictive performance.\n",
    "\n",
    "7. **XG Boost Regressor:**\n",
    "   - A gradient boosting model known for its high predictive power and computational efficiency.\n",
    "\n",
    "8. **Bagging Regressor:**\n",
    "   - Utilizes Bootstrap Aggregating to create an ensemble of multiple decision tree regressors for enhanced prediction accuracy.\n",
    "\n",
    "\n",
    "To determine the best model, I'll test each and assess their performance using metrics like R-squared and Mean Squared Error. The model with the best predictive accuracy will be selected for further optimization.\n",
    "\n",
    "\n",
    "## 4. Which model’s performance is best and what could be the possible reason for that?\n",
    "I evaluated several regression models to predict the target variable, and here are the results:\n",
    "\n",
    "**Linear Regression:**\n",
    "- R-squared (R^2) - Train: 0.5335, Test: 0.5367\n",
    "- Mean Squared Error (MSE) - Train: 75026071.4080, Test: 68751323.9296\n",
    "\n",
    "**Ridge Regression:**\n",
    "- R-squared (R^2) - Train: 0.5335, Test: 0.5366\n",
    "- Mean Squared Error (MSE) - Train: 75028518.5878, Test: 68763923.8062\n",
    "\n",
    "**Lasso Regression:**\n",
    "- R-squared (R^2) - Train: 0.5332, Test: 0.5374\n",
    "- Mean Squared Error (MSE) - Train: 75080196.9627, Test: 68643390.1817\n",
    "\n",
    "**Decision Tree Regression:**\n",
    "- R-squared (R^2) - Train: 0.6137, Test: 0.5928\n",
    "- Mean Squared Error (MSE) - Train: 62125086.3121, Test: 60424631.0261\n",
    "\n",
    "**Decision Tree Regression with GridSearchCV:**\n",
    "- Best Hyperparameters: {'max_depth': 4, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
    "- R-squared (R^2) - Train: 0.6398, Test: 0.6032\n",
    "- Mean Squared Error (MSE) - Train: 57933285.6436, Test: 58875901.6901\n",
    "\n",
    "**Random Forest Regressor:**\n",
    "- Best Hyperparameters: RandomForestRegressor(max_depth=30, n_estimators=300)\n",
    "- R-squared (R^2) - Train: 0.9493, Test: 0.6590\n",
    "- Mean Squared Error (MSE) - Train: 8154715.2179, Test: 50597423.3677\n",
    "\n",
    "**XG Boost Regressor:**\n",
    "- Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 100}\n",
    "- R-squared (R^2) - Train: 0.7758, Test: 0.6261\n",
    "- Mean Squared Error (MSE) - Train: 36064091.1440, Test: 55476872.0819\n",
    "\n",
    "**Bagging Regressor:**\n",
    "- Best Hyperparameters: BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=100, random_state=0)\n",
    "- R-squared (R^2) - Train: 0.9474, Test: 0.6537\n",
    "- Mean Squared Error (MSE) - Train: 8458815.4932, Test: 51388475.1828\n",
    "\n",
    "### Decision and Justification\n",
    "\n",
    "Based on the results, the Random Forest Regressor with the hyperparameters (max_depth=30, n_estimators=300) is the best performer for predicting the target variable.\n",
    "\n",
    "Here are the reasons for choosing this model:\n",
    "\n",
    "1. **R-squared (R^2):** The Random Forest Regressor achieves the highest R-squared value on the test dataset (0.6590) among all the models, indicating its strong predictive power and its ability to explain a significant portion of the variance in the target variable.\n",
    "\n",
    "2. **Mean Squared Error (MSE):** The Random Forest Regressor has the lowest test MSE (50597423.3677) compared to all other models, indicating that its predictions are closest to the actual values and that it provides the best fit for the data.\n",
    "\n",
    "3. **Hyperparameter Optimization:** The Random Forest Regressor has been fine-tuned with a max depth of 30 and 300 estimators, contributing to its outstanding performance.\n",
    "\n",
    "In conclusion, based on the results, I recommend using the Random Forest Regressor with max depth=30 and 300 estimators for your predictive tasks due to its high prediction accuracy.\n",
    "\n",
    "\n",
    "\n",
    "## 5. What steps can you take to improve this selected model’s performance even further?\n",
    "\n",
    "To further enhance the performance of the selected model, I took the following steps:\n",
    "\n",
    "1. **Validation Curve for Ridge and Lasso Regression:**\n",
    "    - For Ridge regression, I used a validation curve to explore different values of alpha. This helped me identify the optimal alpha value that balances bias and variance effectively.\n",
    "    - The best alpha for Ridge regression was found to be approximately 10.72.\n",
    "    \n",
    "    - For Lasso regression, a similar validation curve was employed to determine the best alpha. The optimal alpha for Lasso regression was approximately 86.97.\n",
    "    \n",
    "    - These optimal alpha values are crucial for regularizing the model and preventing overfitting.\n",
    "\n",
    "2. **GridSearchCV for Tree Regression:**\n",
    "    - To further improve the performance of the Decision Tree Regression model, I utilized GridSearchCV. This technique systematically searches through a range of hyperparameters to find the best combination that optimizes model performance.\n",
    "    - The best hyperparameters for the Decision Tree Regression model were identified as follows:\n",
    "        - 'max_depth': 4\n",
    "        - 'min_samples_leaf': 1\n",
    "        - 'min_samples_split': 5\n",
    "    - Random Forest Regressor achieved the best hyperparameters with a max depth of 30 and 300 estimators.\n",
    "     - XG Boost Regressor was optimized with a learning rate of 0.1, max depth of 4, and 100 estimators.\n",
    "      - Bagging Regressor was configured with 100 decision tree estimators.\n",
    "          \n",
    "By performing these steps, I have fine-tuned the model and optimized its hyperparameters to increase its efficiency and accuracy in making predictions.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
